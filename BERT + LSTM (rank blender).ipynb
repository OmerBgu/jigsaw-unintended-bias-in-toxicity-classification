{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Thanks for @christofhenkel @abhishek @iezepov for their great work:\n",
    "\n",
    "https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage\n",
    "https://www.kaggle.com/abhishek/pytorch-bert-inference\n",
    "https://www.kaggle.com/iezepov/starter-gensim-word-embeddings"
   ]
  },
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "package_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\n",
    "sys.path.append(package_dir)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import torch.optim as optim\n",
    "\n",
    "import fastai\n",
    "from fastai.train import Learner\n",
    "from fastai.train import DataBunch\n",
    "from fastai.callbacks import *\n",
    "from fastai.basic_data import DatasetType\n",
    "import fastprogress\n",
    "from fastprogress import force_console_behavior\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for text in tqdm(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    return np.array(all_tokens)\n",
    "\n",
    "def is_interactive():\n",
    "    return 'SHLVL' not in os.environ\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    #with open(path,'rb') as f:\n",
    "    emb_arr = KeyedVectors.load(path)\n",
    "    return emb_arr\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((max_features + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    embedding_matrix[i] = embedding_index[word.lower()]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        embedding_matrix[i] = embedding_index[word.title()]\n",
    "                    except KeyError:\n",
    "                        unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "\n",
    "def train_model(learn,test,output_dim,lr=0.001,\n",
    "                batch_size=512, n_epochs=4,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    \n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    n = len(learn.data.train_dl)\n",
    "    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\n",
    "    sched = GeneralScheduler(learn, phases)\n",
    "    learn.callbacks.append(sched)\n",
    "    for epoch in range(n_epochs):\n",
    "        learn.fit(1)\n",
    "        test_preds = np.zeros((len(test), output_dim))    \n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            X = x_batch[0].cuda()\n",
    "            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n",
    "\n",
    "        all_test_preds.append(test_preds)\n",
    "\n",
    "\n",
    "    if enable_checkpoint_ensemble:\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds\n",
    "\n",
    "\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def preprocess(x):\n",
    "    x = handle_punctuation(x)\n",
    "    x = handle_contractions(x)\n",
    "    x = fix_quote(x)\n",
    "    return x\n",
    "\n",
    "class SequenceBucketCollator():\n",
    "    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "        self.choose_length = choose_length\n",
    "        self.sequence_index = sequence_index\n",
    "        self.length_index = length_index\n",
    "        self.label_index = label_index\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "        \n",
    "        sequences = batch[self.sequence_index]\n",
    "        lengths = batch[self.length_index]\n",
    "        \n",
    "        length = self.choose_length(lengths)\n",
    "        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "        padded_sequences = sequences[:, mask]\n",
    "        \n",
    "        batch[self.sequence_index] = padded_sequences\n",
    "        \n",
    "        if self.label_index is not None:\n",
    "            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "    \n",
    "        return batch\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x, lengths=None):\n",
    "        h_embedding = self.embedding(x.long())\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "def custom_loss(data, targets):\n",
    "    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n",
    "    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n",
    "    return (bce_loss_1 * loss_weight) + bce_loss_2\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ensemble_predictions(predictions, weights, type_=\"linear\"):\n",
    "    assert np.isclose(np.sum(weights), 1.0)\n",
    "    if type_ == \"linear\":\n",
    "        res = np.average(predictions, weights=weights, axis=0)\n",
    "    elif type_ == \"harmonic\":\n",
    "        res = np.average([1 / p for p in predictions], weights=weights, axis=0)\n",
    "        return 1 / res\n",
    "    elif type_ == \"geometric\":\n",
    "        numerator = np.average(\n",
    "            [np.log(p) for p in predictions], weights=weights, axis=0\n",
    "        )\n",
    "        res = np.exp(numerator / sum(weights))\n",
    "        return res\n",
    "    elif type_ == \"rank\":\n",
    "        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n",
    "        return res / (len(res) + 1)\n",
    "    return res"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(action='once')\n",
    "device = torch.device('cuda')\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "SEED = 1234\n",
    "BATCH_SIZE = 512\n",
    "BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "bert_config = BertConfig('../input/bert-inference/bert/bert_config.json')\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "\n",
    "tqdm.pandas()\n",
    "CRAWL_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/crawl-300d-2M.gensim'\n",
    "GLOVE_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/glove.840B.300d.gensim'\n",
    "NUM_MODELS = 2\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220\n",
    "if not is_interactive():\n",
    "    def nop(it, *a, **k):\n",
    "        return it\n",
    "\n",
    "    tqdm = nop\n",
    "\n",
    "    fastprogress.fastprogress.NO_BAR = True\n",
    "    master_bar, progress_bar = force_console_behavior()\n",
    "    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n",
    "\n",
    "seed_everything()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**BERT Part**"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n",
    "test_df['comment_text'] = test_df['comment_text'].astype(str) \n",
    "X_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "model = BertForSequenceClassification(bert_config, num_labels=1)\n",
    "model.load_state_dict(torch.load(\"../input/bert-inference/bert/bert_pytorch.bin\"))\n",
    "model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.eval()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "test_preds = np.zeros((len(X_test)))\n",
    "test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=512, shuffle=False)\n",
    "tk0 = tqdm(test_loader)\n",
    "for i, (x_batch,) in enumerate(tk0):\n",
    "    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n",
    "    test_preds[i * 512:(i + 1) * 512] = pred[:, 0].detach().cpu().squeeze().numpy()\n",
    "\n",
    "test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "submission_bert = pd.DataFrame.from_dict({\n",
    "    'id': test_df['id'],\n",
    "    'prediction': test_pred\n",
    "})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**LSTM Part**"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "train_df = reduce_mem_usage(pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "symbols_to_isolate = '.,?!-;*\"‚Ä¶:‚Äî()%#$&_/@Ôºº„Éªœâ+=‚Äù‚Äú[]^‚Äì>\\\\¬∞<~‚Ä¢‚â†‚Ñ¢Àà ä…í‚àû¬ß{}¬∑œÑŒ±‚ù§‚ò∫…°|¬¢‚ÜíÃ∂`‚ù•‚îÅ‚î£‚î´‚îóÔºØ‚ñ∫‚òÖ¬©‚Äï…™‚úî¬Æ\\x96\\x92‚óè¬£‚ô•‚û§¬¥¬π‚òï‚âà√∑‚ô°‚óê‚ïë‚ñ¨‚Ä≤…îÀê‚Ç¨€©€û‚Ä†Œº‚úí‚û•‚ïê‚òÜÀå‚óÑ¬Ω ªœÄŒ¥Œ∑ŒªœÉŒµœÅŒΩ É‚ú¨Ôº≥ÔºµÔº∞Ôº•Ôº≤Ôº©Ôº¥‚òª¬±‚ôç¬µ¬∫¬æ‚úì‚óæÿüÔºé‚¨Ö‚ÑÖ¬ª–í–∞–≤‚ù£‚ãÖ¬ø¬¨‚ô´Ôº£Ôº≠Œ≤‚ñà‚ñì‚ñí‚ñë‚áí‚≠ê‚Ä∫¬°‚ÇÇ‚ÇÉ‚ùß‚ñ∞‚ñî‚óû‚ñÄ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ÜôŒ≥ÃÑ‚Ä≥‚òπ‚û°¬´œÜ‚Öì‚Äû‚úãÔºö¬•Ã≤ÃÖÃÅ‚àô‚Äõ‚óá‚úè‚ñ∑‚ùì‚ùó¬∂ÀöÀôÔºâ—Å–∏ ø‚ú®„ÄÇ…ë\\x80‚óïÔºÅÔºÖ¬Ø‚àíÔ¨ÇÔ¨Å‚ÇÅ¬≤ å¬º‚Å¥‚ÅÑ‚ÇÑ‚å†‚ô≠‚úò‚ï™‚ñ∂‚ò≠‚ú≠‚ô™‚òî‚ò†‚ôÇ‚òÉ‚òé‚úà‚úå‚ú∞‚ùÜ‚òô‚óã‚Ä£‚öìÂπ¥‚àé‚Ñí‚ñ™‚ñô‚òè‚ÖõÔΩÉÔΩÅÔΩì«Ä‚ÑÆ¬∏ÔΩó‚Äö‚àº‚Äñ‚Ñ≥‚ùÑ‚Üê‚òº‚ãÜ í‚äÇ„ÄÅ‚Öî¬®Õ°‡πè‚öæ‚öΩŒ¶√óŒ∏Ôø¶ÔºüÔºà‚ÑÉ‚è©‚òÆ‚ö†Êúà‚úä‚ùå‚≠ï‚ñ∏‚ñ†‚áå‚òê‚òë‚ö°‚òÑ«´‚ï≠‚à©‚ïÆÔºå‰æãÔºû ï…êÃ£Œî‚ÇÄ‚úû‚îà‚ï±‚ï≤‚ñè‚ñï‚îÉ‚ï∞‚ñä‚ñã‚ïØ‚î≥‚îä‚â•‚òí‚Üë‚òù…π‚úÖ‚òõ‚ô©‚òûÔº°Ôº™Ôº¢‚óî‚ó°‚Üì‚ôÄ‚¨ÜÃ±‚Ñè\\x91‚†ÄÀ§‚ïö‚Ü∫‚á§‚àè‚úæ‚ó¶‚ô¨¬≥„ÅÆÔΩúÔºè‚àµ‚à¥‚àöŒ©¬§‚òú‚ñ≤‚Ü≥‚ñ´‚Äø‚¨á‚úßÔΩèÔΩñÔΩçÔºçÔºíÔºêÔºòÔºá‚Ä∞‚â§‚àïÀÜ‚öú‚òÅ'\n",
    "symbols_to_delete = '\\nüçï\\rüêµüòë\\xa0\\ue014\\t\\uf818\\uf04a\\xadüò¢üê∂Ô∏è\\uf0e0üòúüòéüëä\\u200b\\u200eüòÅÿπÿØŸàŸäŸáÿµŸÇÿ£ŸÜÿßÿÆŸÑŸâÿ®ŸÖÿ∫ÿ±üòçüíñüíµ–ïüëéüòÄüòÇ\\u202a\\u202cüî•üòÑüèªüí•·¥ç è Ä·¥á…¥·¥Ö·¥è·¥Ä·¥ã ú·¥ú ü·¥õ·¥Ñ·¥ò ô“ì·¥ä·¥°…¢üòãüëè◊©◊ú◊ï◊ù◊ë◊ôüò±‚Äº\\x81„Ç®„É≥„Ç∏ÊïÖÈöú\\u2009üöå·¥µÕûüåüüòäüò≥üòßüôÄüòêüòï\\u200füëçüòÆüòÉüòò◊ê◊¢◊õ◊óüí©üíØ‚õΩüöÑüèº‡Æúüòñ·¥†üö≤‚Äêüòüüòàüí™üôèüéØüåπüòáüíîüò°\\x7füëå·ºê·Ω∂ŒÆŒπ·Ω≤Œ∫·ºÄŒØ·øÉ·º¥ŒæüôÑÔº®üò†\\ufeff\\u2028üòâüò§‚õ∫üôÇ\\u3000ÿ™ÿ≠ŸÉÿ≥ÿ©üëÆüíôŸÅÿ≤ÿ∑üòèüçæüéâüòû\\u2008üèæüòÖüò≠üëªüò•üòîüòìüèΩüéÜüçªüçΩüé∂üå∫ü§îüò™\\x08‚Äëüê∞üêáüê±üôÜüò®üôÉüíïùòäùò¶ùò≥ùò¢ùòµùò∞ùò§ùò∫ùò¥ùò™ùòßùòÆùò£üíóüíöÂú∞ÁçÑË∞∑—É–ª–∫–Ω–ü–æ–ê–ùüêæüêïüòÜ◊îüîóüöΩÊ≠åËàû‰ºéüôàüò¥üèøü§óüá∫üá∏–ºœÖ—Ç—ï‚§µüèÜüéÉüò©\\u200aüå†üêüüí´üí∞üíé—ç–ø—Ä–¥\\x95üñêüôÖ‚õ≤üç∞ü§êüëÜüôå\\u2002üíõüôÅüëÄüôäüôâ\\u2004À¢·µí ≥ ∏·¥º·¥∑·¥∫ ∑·µó ∞·µâ·µò\\x13üö¨ü§ì\\ue602üòµŒ¨ŒøœåœÇŒ≠·Ω∏◊™◊û◊ì◊£◊†◊®◊ö◊¶◊òüòíÕùüÜïüëÖüë•üëÑüîÑüî§üëâüë§üë∂üë≤üîõüéì\\uf0b7\\uf04c\\x9f\\x10ÊàêÈÉΩüò£‚è∫üòåü§ëüåèüòØ–µ—Öüò≤·º∏·æ∂·ΩÅüíûüöìüîîüìöüèÄüëê\\u202düí§üçá\\ue613Â∞èÂúüË±Üüè°‚ùî‚Åâ\\u202füë†„Äã‡§ï‡§∞‡•ç‡§Æ‡§æüáπüáºüå∏Ëî°Ëã±Êñáüåûüé≤„É¨„ÇØ„Çµ„ÇπüòõÂ§ñÂõΩ‰∫∫ÂÖ≥Á≥ª–°–±üíãüíÄüéÑüíúü§¢ŸêŸé—å—ã–≥—è‰∏çÊòØ\\x9c\\x9düóë\\u2005üíÉüì£üëø‡ºº„Å§‡ºΩüò∞·∏∑–ó–∑‚ñ±—ÜÔøºü§£ÂçñÊ∏©Âì•ÂçéËÆÆ‰ºö‰∏ãÈôç‰Ω†Â§±ÂéªÊâÄÊúâÁöÑÈí±Âä†ÊãøÂ§ßÂùèÁ®éÈ™óÂ≠êüêù„ÉÑüéÖ\\x85üç∫ÿ¢ÿ•ÿ¥ÿ°üéµüåéÕü·ºîÊ≤πÂà´ÂÖãü§°ü§•üò¨ü§ß–π\\u2003üöÄü§¥ ≤—à—á–ò–û–†–§–î–Ø–ú—é–∂üòùüñë·Ωê·ΩªœçÁâπÊÆä‰ΩúÊà¶Áæ§—âüí®ÂúÜÊòéÂõ≠◊ß‚Ñêüèàüò∫üåç‚èè·ªáüçîüêÆüçÅüçÜüçëüåÆüåØü§¶\\u200dùìíùì≤ùìøùìµÏïàÏòÅÌïòÏÑ∏Ïöî–ñ—ô–ö—õüçÄüò´ü§§·ø¶ÊàëÂá∫ÁîüÂú®‰∫ÜÂèØ‰ª•ËØ¥ÊôÆÈÄöËØùÊ±âËØ≠Â•ΩÊûÅüéºüï∫üç∏ü•ÇüóΩüéáüéäüÜòü§†üë©üñíüö™Â§©‰∏ÄÂÆ∂‚ö≤\\u2006‚ö≠‚öÜ‚¨≠‚¨Ø‚èñÊñ∞‚úÄ‚ïåüá´üá∑üá©üá™üáÆüá¨üáßüò∑üá®üá¶–•–®üåê\\x1fÊùÄÈ∏°ÁªôÁå¥Áúã Åùó™ùóµùó≤ùóªùòÜùóºùòÇùóøùóÆùóπùó∂ùòáùóØùòÅùó∞ùòÄùòÖùóΩùòÑùó±üì∫œñ\\u2000“Ø’Ω·¥¶·é•“ªÕ∫\\u2007’∞\\u2001…©ÔΩôÔΩÖ‡µ¶ÔΩå∆ΩÔΩàùêìùê°ùêûùê´ùêÆùêùùêöùêÉùêúùê©ùê≠ùê¢ùê®ùêß∆Ñ·¥®◊ü·ëØ‡ªêŒ§·èß‡Ø¶–Ü·¥ë‹Åùê¨ùê∞ùê≤ùêõùê¶ùêØùêëùêôùê£ùêáùêÇùêòùüé‘ú–¢·óû‡±¶„Äî·é´ùê≥ùêîùê±ùüîùüìùêÖüêãÔ¨Éüíòüíì—ëùò•ùòØùò∂üíêüåãüåÑüåÖùô¨ùôñùô®ùô§ùô£ùô°ùôÆùôòùô†ùôöùôôùôúùôßùô•ùô©ùô™ùôóùôûùôùùôõüë∫üê∑‚ÑãùêÄùê•ùê™üö∂ùô¢·ºπü§òÕ¶üí∏ÿ¨Ìå®Ìã∞Ôº∑ùôá·µªüëÇüëÉ…úüé´\\uf0a7–ë–£—ñüö¢üöÇ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä·øÜüèÉùì¨ùìªùì¥ùìÆùìΩùìº‚òòÔ¥æÃØÔ¥ø‚ÇΩ\\ue807ùëªùíÜùíçùíïùíâùíìùíñùíÇùíèùíÖùíîùíéùíóùíäüëΩüòô\\u200c–õ‚Äíüéæüëπ‚éåüèí‚õ∏ÂÖ¨ÂØìÂÖªÂÆ†Áâ©ÂêóüèÑüêÄüöëü§∑ÊìçÁæéùíëùíöùíêùë¥ü§ôüêíÊ¨¢ËøéÊù•Âà∞ÈòøÊãâÊñØ◊°◊§ùô´üêàùíåùôäùô≠ùôÜùôãùôçùòºùôÖÔ∑ªü¶ÑÂ∑®Êî∂Ëµ¢ÂæóÁôΩÈ¨ºÊÑ§ÊÄíË¶Å‰π∞È¢ù·∫Ωüöóüê≥ùüèùêüùüñùüëùüïùíÑùüóùê†ùôÑùôÉüëáÈîüÊñ§Êã∑ùó¢ùü≥ùü±ùü¨‚¶Å„Éû„É´„Éè„Éã„ÉÅ„É≠Ê†™ÂºèÁ§æ‚õ∑ÌïúÍµ≠Ïñ¥„Ñ∏„ÖìÎãàÕú ñùòøùôî‚Çµùí©‚ÑØùíæùìÅùí∂ùìâùìáùìäùìÉùìàùìÖ‚Ñ¥ùíªùíΩùìÄùìåùí∏ùìéùôèŒ∂ùôüùòÉùó∫ùüÆùü≠ùüØùü≤üëãü¶äÂ§ö‰º¶üêΩüéªüéπ‚õìüèπüç∑ü¶Ü‰∏∫Âíå‰∏≠ÂèãË∞äÁ•ùË¥∫‰∏éÂÖ∂ÊÉ≥Ë±°ÂØπÊ≥ïÂ¶ÇÁõ¥Êé•ÈóÆÁî®Ëá™Â∑±ÁåúÊú¨‰º†ÊïôÂ£´Ê≤°ÁßØÂîØËÆ§ËØÜÂü∫Áù£ÂæíÊõæÁªèËÆ©Áõ∏‰ø°ËÄ∂Á®£Â§çÊ¥ªÊ≠ªÊÄ™‰ªñ‰ΩÜÂΩì‰ª¨ËÅä‰∫õÊîøÊ≤ªÈ¢òÊó∂ÂÄôÊàòËÉúÂõ†Âú£ÊääÂÖ®Â†ÇÁªìÂ©öÂ≠©ÊÅêÊÉß‰∏îÊ†óË∞ìËøôÊ†∑Ëøò‚ôæüé∏ü§ïü§í‚õëüéÅÊâπÂà§Ê£ÄËÆ®üèùü¶Åüôãüò∂Ï•êÏä§ÌÉ±Ìä∏Î§ºÎèÑÏÑùÏú†Í∞ÄÍ≤©Ïù∏ÏÉÅÏù¥Í≤ΩÏ†úÌô©ÏùÑÎ†µÍ≤åÎßåÎì§ÏßÄÏïäÎ°ùÏûòÍ¥ÄÎ¶¨Ìï¥ÏïºÌï©Îã§Ï∫êÎÇòÏóêÏÑúÎåÄÎßàÏ¥àÏôÄÌôîÏïΩÍ∏àÏùòÌíàÎü∞ÏÑ±Î∂ÑÍ∞àÎïåÎäîÎ∞òÎìúÏãúÌóàÎêúÏÇ¨Ïö©üî´üëÅÂá∏·Ω∞üí≤üóØùôà·ºåùíáùíàùíòùíÉùë¨ùë∂ùïæùñôùñóùñÜùñéùñåùñçùñïùñäùñîùñëùñâùñìùñêùñúùñûùñöùñáùïøùñòùñÑùñõùñíùñãùñÇùï¥ùñüùñàùï∏üëëüöøüí°Áü•ÂΩºÁôæ\\uf005ùôÄùíõùë≤ùë≥ùëæùíãùüíüò¶ùôíùòæùòΩüèêùò©ùò®·Ωº·πëùë±ùëπùë´ùëµùë™üá∞üáµüëæ·ìá·íß·î≠·êÉ·êß·ê¶·ë≥·ê®·ìÉ·ìÇ·ë≤·ê∏·ë≠·ëé·ìÄ·ê£üêÑüéàüî®üêéü§ûüê∏üíüüé∞üåùüõ≥ÁÇπÂáªÊü•Áâàüç≠ùë•ùë¶ùëßÔºÆÔºßüë£\\uf020„Å£üèâ—Ñüí≠üé•Œûüê¥üë®ü§≥ü¶ç\\x0büç©ùëØùííüòóùüêüèÇüë≥üçóüïâüê≤⁄Ü€åùëÆùóïùó¥üçíÍú•‚≤£‚≤èüêë‚è∞ÈâÑ„É™‰∫ã‰ª∂—óüíä„Äå„Äç\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ÁáªË£Ω„Ç∑ËôöÂÅΩÂ±ÅÁêÜÂ±à–ìùë©ùë∞ùíÄùë∫üå§ùó≥ùóúùóôùó¶ùóßüçä·Ω∫·ºà·º°œá·øñŒõ‚§èüá≥ùíôœà’Å’¥’•’º’°’µ’´’∂÷Ä÷Ç’§’±ÂÜ¨Ëá≥·ΩÄùíÅüîπü§öüçéùë∑üêÇüíÖùò¨ùò±ùò∏ùò∑ùòêùò≠ùòìùòñùòπùò≤ùò´⁄©Œíœéüí¢ŒúŒüŒùŒëŒïüá±‚ô≤ùùà‚Ü¥üíí‚äò»ªüö¥üñïüñ§ü•òüìçüëà‚ûïüö´üé®üåëüêªùêéùêçùêäùë≠ü§ñüééüòºüï∑ÔΩáÔΩíÔΩéÔΩîÔΩâÔΩÑÔΩïÔΩÜÔΩÇÔΩãùü∞üá¥üá≠üáªüá≤ùóûùó≠ùóòùó§üëºüìâüçüüç¶üåàüî≠„Ääüêäüêç\\uf10a·Éö⁄°üê¶\\U0001f92f\\U0001f92aüê°üí≥·º±üôáùó∏ùóüùó†ùó∑ü•ú„Åï„Çà„ÅÜ„Å™„Çâüîº'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "x_train = train_df['comment_text'].progress_apply(lambda x:preprocess(x))\n",
    "y_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "x_test = test_df['comment_text'].progress_apply(lambda x:preprocess(x))\n",
    "\n",
    "identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish','muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "# identity_columns = ['psychiatric_or_mental_illness', 'white', 'black',\n",
    "#                          'male', 'female', 'muslim',\n",
    "#                          'transgender', 'homosexual_gay_or_lesbian', \n",
    "#                          'jewish', 'hindu', 'christian'] # new change get : transgender,hindu\n",
    "\n",
    "\n",
    "\n",
    "# Overall\n",
    "weights = np.ones((len(x_train),)) / 4\n",
    "# Subgroup\n",
    "weights += (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "# Background Positive, Subgroup Negative\n",
    "weights += (( (train_df['target'].values>=0.5).astype(bool).astype(np.int) +\n",
    "   (train_df[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# Background Negative, Subgroup Positive\n",
    "weights += (( (train_df['target'].values<0.5).astype(bool).astype(np.int) +\n",
    "   (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "y_train = np.vstack([(train_df['target'].values>=0.5).astype(np.int),weights]).T\n",
    "\n",
    "max_features = 410047"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n‚Äú‚Äù‚Äô\\'‚àûŒ∏√∑Œ±‚Ä¢√†‚àíŒ≤‚àÖ¬≥œÄ‚Äò‚Çπ¬¥¬∞¬£‚Ç¨\\√ó‚Ñ¢‚àö¬≤‚Äî'\n",
    "tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)\n",
    "# new chagnes \n",
    "# tokenizer = text.Tokenizer(num_words = max_features, filters=CHARS_TO_REMOVE,lower=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "print('n unknown words (crawl): ', len(unknown_words_crawl))\n",
    "\n",
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
    "print('n unknown words (glove): ', len(unknown_words_glove))\n",
    "\n",
    "max_features = max_features or len(tokenizer.word_index) + 1\n",
    "max_features\n",
    "\n",
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "embedding_matrix.shape\n",
    "\n",
    "del crawl_matrix\n",
    "del glove_matrix\n",
    "gc.collect()\n",
    "\n",
    "y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n",
    " \n",
    "maxlen = 300\n",
    "x_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n",
    "\n",
    "x_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=maxlen))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "batch_size = 512\n",
    "test_dataset = data.TensorDataset(x_test_padded, test_lengths)\n",
    "train_dataset = data.TensorDataset(x_train_padded, lengths, y_train_torch)\n",
    "valid_dataset = data.Subset(train_dataset, indices=[0, 1])\n",
    "\n",
    "train_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n",
    "                                        sequence_index=0, \n",
    "                                        length_index=1, \n",
    "                                        label_index=2)\n",
    "test_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "\n",
    "databunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader, collate_fn=train_collator)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "all_test_preds = []\n",
    "TEST_LSTM_PARAMS = False # flag for hyper tuning of LSTM \n",
    "if TEST_LSTM_PARAMS:\n",
    "    global NUM_MODELS\n",
    "    NUM_MODELS = 1\n",
    "    lrs = [1e-2, 1e-3, 1e-4]\n",
    "\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    print('Model ', model_idx)\n",
    "    seed_everything(1 + model_idx)\n",
    "    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "    learn = Learner(databunch, model, loss_func=custom_loss)\n",
    "    # learn = Learner(databunch, model, loss_func=custom_loss,opt_func=optim.Adamax)\n",
    "    \n",
    "    test_preds = train_model(learn,test_dataset,output_dim=7,lr=0.001)\n",
    "            \n",
    "    # test_preds = train_model(learn,test_dataset,output_dim=7)    \n",
    "    all_test_preds.append(test_preds)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "submission_lstm = pd.DataFrame.from_dict({\n",
    "    'id': test_df['id'],\n",
    "    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n",
    "})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Blending part**"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "submission = pd.read_csv(\n",
    "    \"../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv\"\n",
    ")\n",
    "\n",
    "# weights = [0.333, 0.667]\n",
    "# weights = [0.4, 0.6] # new chagne\n",
    "weights = [0.38, 0.62] # new chagne\n",
    "submission[\"prediction\"] = ensemble_predictions(\n",
    "    [submission_bert.prediction.values, submission_lstm.prediction.values],\n",
    "    weights,\n",
    "    # type_=\"rank\",\n",
    "    type_=\"linear\", # new chagne \n",
    ")\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}